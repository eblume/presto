#!/usr/bin/env python3

""" Usage: ./presto_makedb output_db.sqlite

Downloads the fuzzworks Eve data sqlite database dump, and prepares it for use
by the presto database. The fuzzworks db is stored at /tmp/eve-asset-db.sqlite
and is left indefinitely, and if it exists it is not re-downloaded. Because of
this, you will need to delete old versions of this file to get the new version.
In general, users do not ever need to use this script - it is generally only to
be used by developers of presto to prepare new asset databases. The presto
database (generated by this script, by a developer) is packaged with presto.
"""
import bz2
import logging
import os.path
import sqlalchemy as sa
import sys
from urllib.request import urlopen

from presto.orm import Base
from presto.map import System, Jump, Constellation, Region
from presto.items import Type, Group, Category, MarketGroup


## Monkeypatch sqlalchemy's sqlite dialect to allow reflection
# I've submitted a patch that's going to merge upstream at some point
# see https://github.com/zzzeek/sqlalchemy/pull/65
from sqlalchemy.dialects.sqlite import base
from sqlalchemy.types import FLOAT, TEXT
base.ischema_names['DOUBLE'] = FLOAT
base.ischema_names['LONGTEXT'] = TEXT


def reflect_db(conn):
    # sqlite3 unicode hack, because eve db has invalid utf-8. Sadface.
    safe_decode = lambda b: b.decode('utf-8', 'ignore')
    conn.connection.connection.text_factory = safe_decode
    meta = sa.MetaData(bind=conn)
    

    meta.reflect()
    return meta

generators = []
def generator(func):
    generators.append(func)
    return func


@generator
def all_marketgroups(conn, tables):
    print("Importing invMarketGroups")
    for mg in fetch(conn, tables["invMarketGroups"]):
        yield MarketGroup(
            id=mg.marketGroupID,
            name=mg.marketGroupName.strip(),  # Strip 'Hull & Armor '
            description=mg.description,
            parent_id=mg.parentGroupID,
        )


@generator
def all_categories(conn, tables):
    print("Importing invCategories")
    for cat in fetch(conn, tables["invCategories"]):
        yield Category(
            id=cat.categoryID,
            name=cat.categoryName,
            description=cat.description,
            published=(cat.published == 1),
        )


@generator
def all_groups(conn, tables):
    print("Importing invGroups")
    for group in fetch(conn, tables["invGroups"]):
        yield Group(
            id=group.groupID,
            name=group.groupName,
            category_id=group.categoryID,
            description=group.description,
            manufacturable=(group.allowManufacture == 1),
            recyclable=(group.allowRecycler == 1),
            anchored=(group.anchored == 1),
            anchorable=(group.anchorable == 1),
            fit_singleton=(group.fittableNonSingleton != 1),
            published=(group.published == 1)
        )


@generator
def all_types(conn, tables):
    print("Importing invTypes")
    for item in fetch(conn, tables["invTypes"]):
        yield Type(
            id=item.typeID,
            group_id=item.groupID,
            name=item.typeName,
            description=item.description,
            mass=item.mass,
            volume=item.volume,
            capacity=item.capacity,
            portionsize=item.portionSize,
            baseprice=item.basePrice,
            published=(item.published == 1),
            marketgroup_id=item.marketGroupID,
        )


@generator
def all_regions(conn, tables):
    print("Importing mapRegions")
    for region in fetch(conn, tables["mapRegions"]):
        yield Region(
            id=region.regionID,
            name=region.regionName,
            x=region.x,
            y=region.y,
            z=region.z,
        )


@generator
def all_constellations(conn, tables):
    print("Importing mapConstellations")
    for constellation in fetch(conn, tables["mapConstellations"]):
        yield Constellation(
            id=constellation.constellationID,
            name=constellation.constellationName,
            x=constellation.x,
            y=constellation.y,
            z=constellation.z,
            region_id=constellation.regionID,
        )


@generator
def all_systems(conn, tables):
    print("Importing mapSolarSystems")
    for system in fetch(conn, tables["mapSolarSystems"]):
        yield System(
            id=system.solarSystemID,
            name=system.solarSystemName,
            x=system.x,
            y=system.y,
            z=system.z,
            constellation_id=system.constellationID,
            region_id=system.regionID,
        )


@generator
def all_jumps(conn, tables):
    print("Importing mapSolarySystemJumps")
    for jump in fetch(conn, tables["mapSolarSystemJumps"]):
        pair = sorted([jump.fromSolarSystemID, jump.toSolarSystemID])
        yield Jump(from_system=pair[0], to_system=pair[1])


def fetch(conn, table):
    return conn.execute(table.select())


def fetch_asset_database():
    """Return a file path pointing to the eve asset datbase.

    This function will look in a hard-coded place for this file. If it is
    there, then the file is returned. If it isn't, the asset database will
    be downloaded and stored at the hard coded place.

    Note that to upgrade the db you will need to delete the old one.
    """
    location = "/tmp/eve-asset-db.sqlite"

    if not os.path.exists(location):
        remote = "https://www.fuzzwork.co.uk/dump/sqlite-latest.sqlite.bz2"
        print("Downloading remote database from {}".format(remote))
        with open(location, 'wb') as db:
            # TODO: Potential exploit when decompressing remote bytes. âˆšsum?
            db.write(bz2.open(urlopen(remote)).read())
    else:
        print("Using existing {}".format(location))

    return location


def main():
    if len(sys.argv) != 2:
        print("Usage: {} OUTPUT_DB.sqlite".format(sys.argv[0]))
        sys.exit(1)

    if os.path.exists(sys.argv[1]):
        print("Database already exists at {}".format(sys.argv[1]))
        sys.exit(2)

    fetched_db = fetch_asset_database()
    
    input_db = sa.create_engine('sqlite:///{}'.format(fetched_db))
    input_conn = input_db.connect()
    output_db = sa.create_engine('sqlite:///{}'.format(sys.argv[1]))
    output_conn = output_db.connect()
    
    # Reflect the input database schema
    input_meta = reflect_db(input_conn)

    # Configure the ouytput database connection
    Session = sa.orm.sessionmaker()
    Session.configure(bind=output_conn)
    output_session = Session()
    
    # Create all the tables we will be populating
    Base.metadata.create_all(output_conn)

    # Run the generators
    for func in generators:
        output_session.add_all(func(input_conn, input_meta.tables))

    # Flush to disk
    output_session.commit()

    input_conn.close()
    output_conn.close()


if __name__ == "__main__":
    main()
